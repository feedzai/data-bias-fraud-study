{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "#\n",
    "# The copyright of this file belongs to Feedzai. The file cannot be\n",
    "# reproduced in whole or in part, stored in a retrieval system,\n",
    "# transmitted in any form, or by any means electronic, mechanical,\n",
    "# photocopying, or otherwise, without the prior permission of the owner.\n",
    "#\n",
    "# (c) 2022 Feedzai, Strictly Confidential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm  # Tested ML method\n",
    "import xgboost as xgb\n",
    "import numpy as np       # Random number generation\n",
    "import seaborn as sns    # Plotting library\n",
    "import pandas as pd      # Read/write data\n",
    "import yaml              # Read hyperparameter space configuration\n",
    "\n",
    "from aequitas.group import Group                # Fairness metrics\n",
    "from matplotlib import pyplot as plt            # Plotting method\n",
    "from sklearn.preprocessing import LabelEncoder  # Categorical encoding for LGBM models\n",
    "from sklearn import metrics                     # ROC metrics\n",
    "from random_search import RandomValueTrial, suggest_callable_hyperparams  # Random search wrapper methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read hyperparameter space for a given algorithm\n",
    "ALGORITHMS = [\"LGBM\", \"XGB\", \"LR\", \"RF\", \"DT\"]\n",
    "\n",
    "hyperparam_spaces = []\n",
    "for a in ALGORITHMS:\n",
    "    with open(f\"../hyperparameter_spaces/{a}.yaml\", \"r\") as file:\n",
    "        hyperparam_spaces.append(yaml.load(file, Loader=yaml.FullLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the desired dataset. To reproduce each scenario in the paper, choose:\n",
    "#   - Base dataset for the baseline\n",
    "#   - Variant I for Scenario 1\n",
    "#   - Variant II for Scenario 2\n",
    "#   - Variant III for Scenario 3\n",
    "#   - Variant V for Scenario 5\n",
    "# Reproducibility for Scenarios 4 and 6 are a work in progress.\n",
    "dataset = pd.read_csv(\"</path/to/dataset>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label field and categorical columns.\n",
    "label = \"fraud_bool\"\n",
    "\n",
    "categorical_features = [\n",
    "    \"payment_type\",\n",
    "    \"employment_status\",\n",
    "    \"housing_status\",\n",
    "    \"source\",\n",
    "    \"device_os\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and test sets. Shuffle data with `sample` method.\n",
    "# The split was done by month. The first 6 months as the train, the last 2 months as test.\n",
    "train_df = dataset[dataset[\"month\"]<6].sample(frac=1, replace=False)\n",
    "test_df = dataset[dataset[\"month\"]>=6].sample(frac=1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical variables in the datasets to integers. \n",
    "# This is expected by LGBM (or columns with the `categorical` data type).\n",
    "for feat in categorical_features:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(train_df[feat])  # Fit an encoder to the train set.\n",
    "    train_df[feat] = encoder.transform(train_df[feat])  # Transform train set.\n",
    "    test_df[feat] = encoder.transform(test_df[feat])    # Transform train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell with train loop.\n",
    "\n",
    "# Define number of trials in Random search.\n",
    "n_trials=50\n",
    "# Seeds for the random search sampling algorithm are the same as the paper.\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "seeds = np.random.randint(10**6, size=n_trials)\n",
    "\n",
    "# Variable to store the results.\n",
    "runs = {}\n",
    "\n",
    "for algo, hyperparam_space, clf in zip(ALGORITHMS, hyperparam_spaces, [lgbm.LGBMClassifier, xgb.XGBClassifier, LogisticRegression, RandomForestClassifier, DecisionTreeClassifier]):\n",
    "    for trial in range(n_trials):\n",
    "        seed = seeds[trial]\n",
    "        trial = RandomValueTrial(seed=seed)\n",
    "        # Hyperparameters for the random search trial.\n",
    "        test_hyperparams = suggest_callable_hyperparams(trial, hyperparam_space)\n",
    "        del test_hyperparams[\"classpath\"] # Remove unnecessary key in hyperparaemters.\n",
    "        \n",
    "        # Update list of tested hyperparameters.\n",
    "        prev_hyperparams = runs.get(\"hyperparams\", [])\n",
    "        prev_hyperparams.append(test_hyperparams)\n",
    "        runs[\"hyperparams\"] = prev_hyperparams\n",
    "    \n",
    "        # Instantiate model\n",
    "        model = clf(n_jobs=-1, **test_hyperparams)\n",
    "\n",
    "        X_train = train_df.drop(columns=[\"fraud_bool\"])\n",
    "        y_train = train_df[\"fraud_bool\"]\n",
    "        X_test = test_df.drop(columns=[\"fraud_bool\"])\n",
    "        y_test = test_df[\"fraud_bool\"]\n",
    "        # Fit model to training data.\n",
    "        if algo == \"LGBM\":\n",
    "            model.fit(X_train, y_train, categorical_feature=categorical_features)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        # Obtain predictions in test data.\n",
    "        predictions = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "        # Obtain ROC curve for the predictions.\n",
    "        fprs, tprs, thresholds = metrics.roc_curve(y_test, predictions)\n",
    "        # Obtain threshold and recall. We select 5% FPR as threshold as in the paper.\n",
    "        threshold = np.min(thresholds[fprs==max(fprs[fprs < 0.05])])\n",
    "        recall = np.max(tprs[fprs==max(fprs[fprs < 0.05])])\n",
    "    \n",
    "        # Binarize predictions for Aequitas.\n",
    "        preds_binary = (predictions > threshold).astype(int)\n",
    "            \n",
    "        # Create a dataframe with protected group column, predictions and labels.\n",
    "        # Here, we select age>=50 as threshold as in the paper.\n",
    "        aequitas_df = pd.DataFrame(\n",
    "            {\n",
    "                \"age\": (X_test[\"customer_age\"]>=50).map({True: \"Older\", False: \"Younger\"}),\n",
    "                \"preds\": preds_binary,\n",
    "                \"y\": y_test.values\n",
    "            }\n",
    "        )\n",
    "            \n",
    "        # Obtain FPR results for different groups.\n",
    "        g = Group()\n",
    "        aequitas_results = g.get_crosstabs(aequitas_df, attr_cols=[\"age\"], score_col=\"preds\", label_col=\"y\")[0]\n",
    "        \n",
    "        # Store the results for the trained model\n",
    "        results = {}\n",
    "        # Performance metric used throughout the paper\n",
    "        results[\"recall\"] = recall\n",
    "        # In the paper, we also collect group-wise comparisons of FNR and Precision.\n",
    "        for m in [\"fpr\", \"fnr\", \"ppv\"]:\n",
    "            m_older = aequitas_results[aequitas_results[\"attribute_value\"] == \"Older\"][[m]].values[0][0]\n",
    "            m_young = aequitas_results[aequitas_results[\"attribute_value\"] == \"Younger\"][[m]].values[0][0]\n",
    "            results[f\"{m}_ratio\"] = min(m_older, m_young) / max(m_older, m_young)\n",
    "               \n",
    "        # Store the results in the runs variable\n",
    "        prev_runs = runs.get(algo, []) # Dataset name depends on scenario being run\n",
    "        prev_runs.append(results)\n",
    "        runs[algo] = prev_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the results for each model in each dataset.\n",
    "rs_results = pd.DataFrame(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to obtain the metric values for a given model.\n",
    "def get_results(results, metric, algo_name):\n",
    "    col = results[algo_name]\n",
    "    values = []\n",
    "    for idx, val in col.iteritems():\n",
    "        values.append(val[metric])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the relevant metrics to plots from the dataframe.\n",
    "FAIRNESS_METRIC_TO_PLOT = \"fpr_ratio\"\n",
    "plot_results = {\"Algorithm\": [], \"Recall\": [], \"FPR Ratio\": []}\n",
    "\n",
    "for a in ALGORITHMS:\n",
    "    plot_results[\"Recall\"] += get_results(rs_results, \"recall\")\n",
    "    plot_results[\"FPR Ratio\"] += get_results(rs_results, FAIRNESS_METRIC_TO_PLOT)\n",
    "\n",
    "    plot_results[\"Algorithm\"] = [a] * len(plot_results[\"FPR Ratio\"])\n",
    "\n",
    "# Create a dataframe for easier plots.\n",
    "plot_results = pd.DataFrame(plot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot with the full results of the random search algorithm.\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\", {\"grid.linestyle\": \"--\"})\n",
    "\n",
    "sns.jointplot(data=plot_results, x=\"Recall\", y=\"FPR Ratio\", hue=\"Algorithm\")\n",
    "plt.ylim((0,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot that the cell above outputs should be an example of one of the experiments ran in the paper, for all the algorithms and HP configurations used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b03de8ef13ec037dc99cfc47bf11e1a24341a64838c0afae640e91f92eed5a45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
